{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\shaur\\Downloads\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Change the current working directory\n",
    "new_directory = \"C:/Users/shaur/Downloads\"\n",
    "os.chdir(new_directory)\n",
    "\n",
    "# Verify the change\n",
    "print(\"Current working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModel.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The black cat sat on the couch and the brown dog slept on the rug.\"\n",
    "tokenzed_text = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutput(last_hidden_state=tensor([[[-0.1878, -0.0283, -0.1687,  ..., -0.1363,  0.7483,  0.1205],\n",
      "         [-0.2751,  0.0506, -0.4477,  ...,  0.1321,  1.3671, -0.4097],\n",
      "         [ 0.0966,  0.1944, -0.0447,  ..., -0.4726,  0.7158, -0.3971],\n",
      "         ...,\n",
      "         [ 0.1167,  0.0275, -0.0336,  ..., -0.2981,  0.5688, -0.2564],\n",
      "         [ 0.6191,  0.1539, -0.6147,  ...,  0.0474, -0.1249, -0.5419],\n",
      "         [ 0.2106,  0.3435, -0.1342,  ..., -0.1020,  0.3287, -0.4919]]],\n",
      "       device='cuda:0'), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "inputs = {k:v.to(device) for k,v in tokenzed_text.items()}\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "black = outputs.last_hidden_state[:,2,:]\n",
    "brown = outputs.last_hidden_state[:,10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90681314"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    return dot(a, b)/(norm(a)*norm(b))\n",
    "cos_sim(black.cpu().squeeze(0).numpy(),brown.cpu().squeeze(0).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Input : 3 inputs, d_model=4\n",
      "[[1. 0. 1. 0.]\n",
      " [0. 2. 0. 2.]\n",
      " [1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 1: Input : 3 inputs, d_model=4\")\n",
    "x =np.array([[1.0, 0.0, 1.0, 0.0], # Input 1\n",
    "[0.0, 2.0, 0.0, 2.0], # Input 2\n",
    "[1.0, 1.0, 1.0, 1.0]]) # Input 3\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: weights 3 dimensions x d_model=4\n",
      "w_query\n",
      "[[1 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 2: weights 3 dimensions x d_model=4\")\n",
    "print(\"w_query\")\n",
    "w_query =np.array([[1, 0, 1],\n",
    "[1, 0, 0],\n",
    "[0, 0, 1],\n",
    "[0, 1, 1]])\n",
    "print(w_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_key\n",
      "[[0 0 1]\n",
      " [1 1 0]\n",
      " [0 1 0]\n",
      " [1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"w_key\")\n",
    "w_key =np.array([[0, 0, 1],\n",
    "[1, 1, 0],\n",
    "[0, 1, 0],\n",
    "[1, 1, 0]])\n",
    "print(w_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_value\n",
      "[[0 2 0]\n",
      " [0 3 0]\n",
      " [1 0 3]\n",
      " [1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"w_value\")\n",
    "w_value = np.array([[0, 2, 0],\n",
    "[0, 3, 0],\n",
    "[1, 0, 3],\n",
    "[1, 1, 0]])\n",
    "print(w_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Matrix multiplication to obtain Q,K,V\n",
      "Query: x * w_query\n",
      "[[1. 0. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 1. 3.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 3: Matrix multiplication to obtain Q,K,V\")\n",
    "print(\"Query: x * w_query\")\n",
    "Q=np.matmul(x,w_query)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: x * w_key\n",
      "[[0. 1. 1.]\n",
      " [4. 4. 0.]\n",
      " [2. 3. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Key: x * w_key\")\n",
    "K=np.matmul(x,w_key)\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: x * w_value\n",
      "[[1. 2. 3.]\n",
      " [2. 8. 0.]\n",
      " [2. 6. 3.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Value: x * w_value\")\n",
    "V=np.matmul(x,w_value)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Scaled Attention Scores\n",
      "[[ 2.  4.  4.]\n",
      " [ 4. 16. 12.]\n",
      " [ 4. 12. 10.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 4: Scaled Attention Scores\")\n",
    "k_d = 1 #square root of k_d=3 rounded down to 1 for this example\n",
    "attention_scores = (Q @ K.transpose())/k_d\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Scaled softmax attention_scores for each vector\n",
      "[0.06337894 0.46831053 0.46831053]\n",
      "[6.03366485e-06 9.82007865e-01 1.79861014e-02]\n",
      "[2.95387223e-04 8.80536902e-01 1.19167711e-01]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 5: Scaled softmax attention_scores for each vector\")\n",
    "attention_scores[0]=softmax(attention_scores[0])\n",
    "attention_scores[1]=softmax(attention_scores[1])\n",
    "attention_scores[2]=softmax(attention_scores[2])\n",
    "print(attention_scores[0])\n",
    "print(attention_scores[1])\n",
    "print(attention_scores[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: attention value obtained by score1/k_d * V\n",
      "[1. 2. 3.]\n",
      "[2. 8. 0.]\n",
      "[2. 6. 3.]\n",
      "Attention 1\n",
      "[0.06337894 0.12675788 0.19013681]\n",
      "Attention 2\n",
      "[0.93662106 3.74648425 0.        ]\n",
      "Attention 3\n",
      "[0.93662106 2.80986319 1.40493159]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 6: attention value obtained by score1/k_d * V\")\n",
    "print(V[0])\n",
    "print(V[1])\n",
    "print(V[2])\n",
    "print(\"Attention 1\")\n",
    "attention1=attention_scores[0].reshape(-1,1)\n",
    "attention1=attention_scores[0][0]*V[0]\n",
    "print(attention1)\n",
    "print(\"Attention 2\")\n",
    "attention2=attention_scores[0][1]*V[1]\n",
    "print(attention2)\n",
    "print(\"Attention 3\")\n",
    "attention3=attention_scores[0][2]*V[2]\n",
    "print(attention3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06337894],\n",
       "       [0.46831053],\n",
       "       [0.46831053]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores[0].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06337894, 0.12675788, 0.19013681])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores[0][0]*V[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.21k/1.21k [00:00<?, ?B/s]\n",
      "c:\\Users\\shaur\\anaconda3\\envs\\dennis\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shaur\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading model.safetensors: 100%|██████████| 892M/892M [01:20<00:00, 11.1MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 147/147 [00:00<?, ?B/s] \n",
      "Downloading (…)ve/main/spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 3.44MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.39M/1.39M [00:00<00:00, 4.06MB/s]\n",
      "c:\\Users\\shaur\\anaconda3\\envs\\dennis\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': \"Il est facile de traduire des langues à l'aide de transformateurs\"}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "translator = pipeline(\"translation_en_to_fr\")\n",
    "#One line of code!\n",
    "print(translator(\"It is easy to translate languages with transformers\",\n",
    "max_length=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dennis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
